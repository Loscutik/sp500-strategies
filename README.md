# sp500-strategies
## Task's description

The goal of the project is  to create a financial strategy that uses a signal generated by a machine learning model to outperform the SP500.
Data processing and feature engineering: Build a dataset: insightful features and the target
Machine Learning pipeline: Train machine learning models on the dataset, select the best model and generate the machine learning signal.
Strategy backtesting: Generate a strategy from the Machine Learning model output and backtest the strategy. 

## Links
[subject] (https://github.com/01-edu/public/tree/master/subjects/ai/sp500-strategies)

[audit] (https://github.com/01-edu/public/tree/master/subjects/ai/sp500-strategies/audit)


## Project's structure
- **data** - downloaded data.
- **environments** - list of the libraries to create a virtual environment.
- **notebooks** - Jupiter notebooks.
- **results** - plots and text files created by the notebooks.
- **scripts** - functions used in the notebooks.

Task's solution is in 2 notebooks:
**sp500-strategies.ipynb** - EDA and dataset creating;
**model_selecting.ipynb** - selection ML model and strategy backtesting.

Overall results is in `/results/report.md`

### sp500-strategies notebook
The notebook describes data analizing and feature engineering to build a dataset. Plots are saved into 'results' directory to lessen the notebook's size.

#### Data complementarity and consistency
- Missing data handling:
 - in `volume` column - replace with the mean of the neighbours;
 - in `open`, `high` and `low` with the value in `close` column.
- For each company analyse periods during which it presents in the list.

#### Outliers
Z-score is used to detect outliers, taking into account the tendency for the price to increase over time. 
Outliers are replaced: 
- 'Open' and  'Close' - with mean of the other prices(if they are not out o the limits);
- 'Low' - with minimum of the oters;
- 'High' - with the maximum.
'Volume' outliers are capped using IQR method.
OutliersTransformer is written to perform the task.

#### Features engineering
- Divide the set into train and test (after 2017-01-01) subtest.
- Create features using Technical Analysis library. 
- Use correlation to choose featues to keep. 
- Cap outliers in those features using IQR method. 
- Choose a transformer for each feature to handle skewed data.

### model_selecting notebook
The notebook describes an ML model selection and the strategy backtesting. Plots are saved into 'results' directory to lessen the notebook's size.


#### Cross validation
- Determine train and validation folds so that the validation fold covers the given test set.
- MultiTimeSeriesSplit class (inherent from sklearn TimeSeriesSplit) is defined to split a data set by time with respect to companies, 
e.g. it puts data for *each* company over a certain period of time into one fold.

#### Grid search
In purpose to select the best model, grid searches are run on pipelines that combine from several classifiers, transformers and demention reducers.
Grid search runs several times for different combinations of 
- transformers 
- estimators
- cross validation folds' size
- the train fold's size restricts.
Function `make_pipeline`, `scorer` and `train_classifiers` are defined to unify pipeline creating and grid searching.

The results of each grid search stage are analysed, compared to the previous stage's results and saved.

`/results/key_results.md` file keeps for each stage and each estimator:
- Parameters grid
- Best parameters
- Best scores
- Search time

`/results/ml_metrics_train.md` file keeps scores on splits for the best pipeline of each stage.

`/results/ml_metrics_train_accuracy_roc.md` keeps *accuracy* and *roc_auc* on splits which showed the most unpredictable results.

Pipelines are created from the next estimators:

**Classifiers**:
- LogisticRegression
- GaussianNB
- RandomForestClassifier
- HistGradientBoostingClassifier
- SGDClassifier

**Demention reducers**:
- FastICA
- passthrough (no reudcers)

**Transformers**:

To *encode* companies name:
- OrdinalEncoder
- TargetEncoder
- BinaryEncoder
- 'drop'

To handle *numerical* data: 
- standardization
  - StandardScaler
  - MaxAbsScaler
- mapping to a Gaussian distribution
  - PowerTransformer
  - QuantileTransformer
- KBinsDiscretizer

The transformers are combined in several ways taking into account the nature of features and the results of skewed data analysis. 

To lessen fitting time, the first stages are fit on a *subset* from the data set. The subset comprises from 40 randomly selected companies. 

The stage which searches parameters for LogisticRegression and RandomForestClassifier are fit on the *whole set*.

**RandomForestClassifier** is the chosen classifiers and the last stage searches parameters for it on the *whole set*. 
Chosen cross validation is **Blocking TimeSeriesSplit** with 11 folds 25 days long each.

File `/results/selected_model.txt` describes the parametes of the chosen model.

#### Chosen model analysis
Apply TunedThresholdClassifierCV to the chosen model.
Compare scores on the train set between the 'pure' chosen model and models with fixed threasholds.

Discover Feature Importance using:
- RandomForestClassifier.feature_importances_
- Partial Dependence Plots
- SHAP Values
- Pair Plot

Use results of the feature analysis to create and fit pipelines with RandomForestClassifier as the estimators and with different transformers and dimention reducers.


#### Strategy backtesting
Create a strategy from the model output and compare PnL from the strategy to PnL from SP500 on the test period (after 2017-01-01).
- Concate the train and test sets;
- Apply the chosen cross validation to the joined set;
- Create the prediction loop, which fits on a train folds and predict on tests folds, i.e. 
    
    the first fit is on train data only,  
    then predict on the first days (25) from the test set,  
    then next fit fold is composed of train data (part of) and the previous test data,  
    and so on.

Create signals for strategies:
- Long Only (take a posion if probability is greater a threshold)
- Long Only proportional (Weights proportional to the probability)
- Short Only (opposite to Long)
- Short Only proportional
- Long Short (take a posion if probability is greater a threshold, sell a posion if probability is less than a threshold)
- Long Short 2 thresholds (2 thresholds for short and long parts, do nothing if the pothion's probability is between the thresholds)

Compare PnL between the strategies and to the return from SP500 history.


## Running 
If you use Python virtual environment, you can set up the one from a file in the `environments` directory.
If you don't work with virtual environments or do not want to, you can use Docker image.
Or you can just read html version of the notebooks which you can find in results/html_notebook directory. 

### Virtual environment 

The virtual environment uses Python >=3.12 with the libraries:
  - jupyter
  - numpy
  - pandas
  - matplotlib
  - seaborn
  - tabulate
  - scikit-learn==1.5.1
  - ta
  - scipy==1.14.0


1. Create an identical environment

   - using conda 
       - (creates an environment named 'sklearn'):
       `conda env create -f environment.yml`
       - activate the environment:
       `conda activate sklearn`
   OR
   - using pip:
     `pip install -r requirements.txt`

2. Run the notebook.

  `jupyter lab notebook/sp500-strategies.ipynb`
  and then
  `jupyter lab notebook/model_selecting.ipynb`

  Be aware that the last notebook takes very long time to run, because I can not to put to GitLab .pkl files for all models that I tried to train due to their size. Only the final model's file is on GitLab repository.
  
  - Key results (like parameters, scores, time) of trained models are in **key_results.md** 
  - Scores on splits are in **ml_metrics_train.md**
  - **report.md** describes the selected model and strategy

  Unfortunatly it is impossible to save all the plots inside of the notebooks, because it increaces their size so that they can not be stored in GitLab. Most of the plots are in `result` directory.


### Docker
Run a script:
- on Windows:    `docker_run.bat`
- on Linux:      `docker_run.sh`
    
- If you use WSL on Windows 11, you can encounter the error "UtilAcceptVsock:250: accept4 failed 110" (I did :) ).
In this case run 
  - `pre_docker_run_4WSL_win11.sh`
   and then
  - `docker_run.sh`
    
If it is needed, give give execute permissions to the scripts:
- `chmod +x pre_docker_run_4WSL_win11.sh`
- `chmod +x docker_run.sh`

After the container runs, the script will show url to open jupyter in your browser. 
If it will not, in another terminal run `docker exec c-emotions jupyter server list`.

In jupyter on the left side you will see files list.
You can find the notebook `trainig CNN.ipynb` in `notebook` directory.

## Author

Olena Budarahina ([obudarah](https://01.kood.tech/git/obudarah))